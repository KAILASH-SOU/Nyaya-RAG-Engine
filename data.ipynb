{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update system packages and install Tesseract for OCR and Poppler for PDF handling\n",
    "!sudo apt-get update\n",
    "!sudo apt-get install -y tesseract-ocr poppler-utils\n",
    "\n",
    "# Install the required Python libraries for extraction, cleaning, and chunking\n",
    "!pip install PyPDF2 pytesseract pdf2image langchain sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Step 1.2: Imports and Function Definitions\n",
    "# This cell contains all the code for our processing logic.\n",
    "# Running this cell just defines the functions; it does not start the main task yet.\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import PyPDF2\n",
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import nltk\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "BASE_DIRECTORY = \"/kaggle/input/legal-dataset-sc-judgments-india-19502024/supreme_court_judgments/\"\n",
    "# We will save batched results here\n",
    "BATCH_OUTPUT_DIR = \"/kaggle/working/chunked_batches/\"\n",
    "# Final merged file\n",
    "FINAL_OUTPUT_PATH = \"/kaggle/working/chunked_legal_data.json\"\n",
    "\n",
    "# Download NLTK data if needed\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "except LookupError:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "\n",
    "# --- FUNCTION DEFINITIONS ---\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"Extracts raw text from a PDF, using OCR as a fallback.\"\"\"\n",
    "    raw_text = \"\"\n",
    "    try:\n",
    "        with open(pdf_path, 'rb') as f:\n",
    "            pdf_reader = PyPDF2.PdfReader(f)\n",
    "            if pdf_reader.is_encrypted:\n",
    "                return \"\"\n",
    "            for page in pdf_reader.pages:\n",
    "                raw_text += page.extract_text() or \"\"\n",
    "    except Exception:\n",
    "        raw_text = \"\"\n",
    "    \n",
    "    if len(raw_text.strip()) < 200:\n",
    "        try:\n",
    "            images = convert_from_path(pdf_path)\n",
    "            ocr_text = \"\".join([pytesseract.image_to_string(img) for img in images])\n",
    "            return ocr_text\n",
    "        except Exception:\n",
    "            return \"\"\n",
    "    return raw_text\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Cleans the raw text.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'indian kanoon - http://indiankanoon\\.org/doc/\\d+/', '', text)\n",
    "    text = re.sub(r'[^a-z0-9\\s.,]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Initialize the text splitter globally\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "print(\" Functions are defined and ready to use.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üöÄ Step 1.3: Main Processing - Process Data Year by Year\n",
    "# This is the main workload. It will process one year at a time and save the results.\n",
    "# If you stop and restart the notebook, it will automatically skip the completed years.\n",
    "\n",
    "print(\"Starting batch processing of all years...\")\n",
    "os.makedirs(BATCH_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "year_directories = sorted(os.listdir(BASE_DIRECTORY))\n",
    "\n",
    "for year_dir in year_directories:\n",
    "    year_path = os.path.join(BASE_DIRECTORY, year_dir)\n",
    "    output_file_path = os.path.join(BATCH_OUTPUT_DIR, f\"chunked_{year_dir}.json\")\n",
    "\n",
    "    # Skip if this year has already been processed\n",
    "    if os.path.exists(output_file_path):\n",
    "        print(f\"--- Year {year_dir} already processed. Skipping. ---\")\n",
    "        continue\n",
    "\n",
    "    if not os.path.isdir(year_path):\n",
    "        continue\n",
    "\n",
    "    print(f\"--- Processing Year: {year_dir} ---\")\n",
    "    yearly_data = {}\n",
    "    \n",
    "    for filename in os.listdir(year_path):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            full_path = os.path.join(year_path, filename)\n",
    "            relative_path = os.path.relpath(full_path, BASE_DIRECTORY)\n",
    "            \n",
    "            print(f\"-> Processing file: {filename}\")\n",
    "            raw_text = extract_text_from_pdf(full_path)\n",
    "            \n",
    "            if raw_text and len(raw_text.strip()) > 100:\n",
    "                cleaned_text = clean_text(raw_text)\n",
    "                chunks = text_splitter.split_text(cleaned_text)\n",
    "                if chunks:\n",
    "                    yearly_data[relative_path] = chunks\n",
    "                    print(f\"  [‚úì] Processed and split into {len(chunks)} chunks.\")\n",
    "            else:\n",
    "                print(f\"  [‚úó] No usable text found in {filename}.\")\n",
    "\n",
    "    # Save the result for the current year\n",
    "    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(yearly_data, f, indent=4)\n",
    "    print(f\"‚úÖ Saved results for year {year_dir} to {output_file_path}\\n\")\n",
    "\n",
    "print(\"\\nüéâ All years have been processed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üì¶ Step 1.4: Merge All Batch Files\n",
    "# This cell reads all the individual JSON files created above\n",
    "# and combines them into one final file for the next stage of the project.\n",
    "\n",
    "print(f\"Merging all batch files from '{BATCH_OUTPUT_DIR}'...\")\n",
    "all_chunked_data = {}\n",
    "\n",
    "for filename in sorted(os.listdir(BATCH_OUTPUT_DIR)):\n",
    "    if filename.endswith(\".json\"):\n",
    "        file_path = os.path.join(BATCH_OUTPUT_DIR, filename)\n",
    "        print(f\"-> Merging {filename}\")\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            yearly_data = json.load(f)\n",
    "            all_chunked_data.update(yearly_data)\n",
    "\n",
    "# Save the final merged dictionary\n",
    "with open(FINAL_OUTPUT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(all_chunked_data, f, indent=4)\n",
    "\n",
    "print(f\"\\n‚úÖ All data merged successfully into '{FINAL_OUTPUT_PATH}'\")\n",
    "print(f\"Total documents processed: {len(all_chunked_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß† Step 2 (Corrected for a Single Notebook Workflow)\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import os # Import os to check if the file exists\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# IMPORTANT: Since you are in the same notebook, we now point directly to the\n",
    "# output file created by your Step 1 script in the /kaggle/working/ directory.\n",
    "CHUNKED_DATA_PATH = \"/kaggle/working/chunked_legal_data.json\"\n",
    "\n",
    "# Output paths for the files we will create\n",
    "INDEX_PATH = \"/kaggle/working/judgments.index\"\n",
    "MAPPING_PATH = \"/kaggle/working/index_to_chunk_map.json\"\n",
    "\n",
    "# The pre-trained model for creating embeddings\n",
    "MODEL_NAME = 'all-MiniLM-L6-v2'\n",
    "\n",
    "# --- 1. VERIFY AND LOAD DATA ---\n",
    "print(\"-> Checking for the chunked data file...\")\n",
    "if not os.path.exists(CHUNKED_DATA_PATH):\n",
    "    print(f\"[‚ùå] ERROR: File not found at '{CHUNKED_DATA_PATH}'.\")\n",
    "    print(\"Please make sure you have successfully run the Step 1 script in a cell above this one.\")\n",
    "else:\n",
    "    print(\"-> File found! Loading chunked data...\")\n",
    "    with open(CHUNKED_DATA_PATH, 'r') as f:\n",
    "        chunked_data = json.load(f)\n",
    "\n",
    "    # --- 2. PREPARE AND FLATTEN DATA ---\n",
    "    all_chunks_text = []\n",
    "    index_to_chunk_map = []\n",
    "    print(\"-> Preparing and flattening data for indexing...\")\n",
    "    for doc_name, chunks in chunked_data.items():\n",
    "        for chunk_text in chunks:\n",
    "            all_chunks_text.append(chunk_text)\n",
    "            index_to_chunk_map.append({\n",
    "                \"doc_name\": doc_name,\n",
    "                \"chunk_text\": chunk_text\n",
    "            })\n",
    "    print(f\"-> Total chunks to be indexed: {len(all_chunks_text)}\")\n",
    "\n",
    "    # --- 3. CREATE EMBEDDINGS ---\n",
    "    print(f\"-> Loading embedding model: {MODEL_NAME}\")\n",
    "    model = SentenceTransformer(MODEL_NAME)\n",
    "    print(\"-> Creating embeddings for all chunks... (This will take a significant amount of time)\")\n",
    "    chunk_embeddings = model.encode(all_chunks_text, show_progress_bar=True, convert_to_numpy=True)\n",
    "    embedding_dim = chunk_embeddings.shape[1]\n",
    "    print(f\"-> Embeddings created with dimension: {embedding_dim}\")\n",
    "\n",
    "    # --- 4. BUILD FAISS INDEX ---\n",
    "    print(\"-> Building FAISS index...\")\n",
    "    index = faiss.IndexFlatL2(embedding_dim)\n",
    "    faiss.normalize_L2(chunk_embeddings)\n",
    "    index.add(chunk_embeddings)\n",
    "    print(f\"-> FAISS index built. Total vectors in index: {index.ntotal}\")\n",
    "\n",
    "    # --- 5. SAVE THE INDEX AND MAPPING ---\n",
    "    print(f\"-> Saving FAISS index to {INDEX_PATH}\")\n",
    "    faiss.write_index(index, INDEX_PATH)\n",
    "    print(f\"-> Saving the chunk mapping file to {MAPPING_PATH}\")\n",
    "    with open(MAPPING_PATH, 'w') as f:\n",
    "        json.dump(index_to_chunk_map, f, indent=4)\n",
    "    print(\"\\n[‚úÖ] Step 2 Complete! Your vector index is ready in the '/kaggle/working/' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Step 3.1: Install App Libraries\n",
    "# Streamlit is for building the web interface.\n",
    "# Pyngrok is a tool to create a public URL for our app running inside Kaggle.\n",
    "\n",
    "!pip install streamlit pyngrok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "print(\"-> Zipping just the large index_to_chunk_map.json file...\")\n",
    "\n",
    "# Define the file you want to zip\n",
    "json_path = \"/kaggle/working/index_to_chunk_map.json\"\n",
    "\n",
    "# Define the name of the new zip file\n",
    "zip_path = \"/kaggle/working/mapping_file_only.zip\"\n",
    "\n",
    "if os.path.exists(json_path):\n",
    "    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
    "        print(f\"    - Compressing {os.path.basename(json_path)}... (This may take a moment)\")\n",
    "        # Add the file to the zip. \n",
    "        # os.path.basename(json_path) makes it so it doesn't store the full /kaggle/working/ path\n",
    "        zipf.write(json_path, os.path.basename(json_path))\n",
    "    \n",
    "    print(f\"\\n[‚úÖ] Compression complete!\")\n",
    "    print(f\"You can now download the smaller '{os.path.basename(zip_path)}' file.\")\n",
    "\n",
    "else:\n",
    "    print(f\"[‚ùå] ERROR: File not found at '{json_path}'.\")\n",
    "    print(\"Please make sure you have successfully run the 'Step 2: Create and Save FAISS Index' cell first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
